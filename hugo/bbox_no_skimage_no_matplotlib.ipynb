{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for trainval.txt in: /Users/paulchainieux/Documents/UCL/ADL/WeakSegNet/hugo/data/annotations/trainval.txt\n",
      "Does the file exists? True\n",
      "\n",
      "----Loading data...\n",
      "Loading split from: /Users/paulchainieux/Documents/UCL/ADL/WeakSegNet/hugo/data/annotations/trainval.txt\n",
      "Loading split from: /Users/paulchainieux/Documents/UCL/ADL/WeakSegNet/hugo/data/annotations/trainval.txt\n",
      "Loading split from: /Users/paulchainieux/Documents/UCL/ADL/WeakSegNet/hugo/data/annotations/test.txt\n",
      "[Data loaded succesfully]\n",
      "\n",
      "Training set: 2944 samples\n",
      "Validation set: 736 samples\n",
      "Test set: 3669 samples\n",
      "Batch number: 1\n",
      "Dice score is 0.505004857471083 || Pixel score is 0.7352609634399414 || IOU score is 0.3946094413756258 || Precision score is 0.8754364333226909 || Recall score is 0.4150360746486448\n",
      "Images saved to file\n",
      "Batch number: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 302\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (image_batch, mask_batch, info_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 302\u001b[0m     cams \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_cams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     bboxs \u001b[38;5;241m=\u001b[39m generate_bboxs(cams, image_batch)\n\u001b[1;32m    304\u001b[0m     pseudo_masks \u001b[38;5;241m=\u001b[39m generate_pseudo_masks(bboxs, image_batch, variant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrabCut\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 236\u001b[0m, in \u001b[0;36mgenerate_cams\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_cams\u001b[39m(images):\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mgenerate_cam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverse_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "Cell \u001b[0;32mIn[2], line 194\u001b[0m, in \u001b[0;36mgenerate_cam\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_cam\u001b[39m(image):\n\u001b[1;32m    193\u001b[0m     image_uint8 \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 194\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    196\u001b[0m     target_layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayer4[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:763\u001b[0m, in \u001b[0;36mresnet50\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m weights \u001b[38;5;241m=\u001b[39m ResNet50_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBottleneck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:298\u001b[0m, in \u001b[0;36m_resnet\u001b[0;34m(block, layers, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     _ovewrite_named_param(kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(weights\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m--> 298\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(weights\u001b[38;5;241m.\u001b[39mget_state_dict(progress\u001b[38;5;241m=\u001b[39mprogress, check_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:210\u001b[0m, in \u001b[0;36mResNet.__init__\u001b[0;34m(self, block, layers, num_classes, zero_init_residual, groups, width_per_group, replace_stride_with_dilation, norm_layer)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mConv2d):\n\u001b[0;32m--> 210\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfan_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnonlinearity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, (nn\u001b[38;5;241m.\u001b[39mBatchNorm2d, nn\u001b[38;5;241m.\u001b[39mGroupNorm)):\n\u001b[1;32m    212\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mconstant_(m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/init.py:571\u001b[0m, in \u001b[0;36mkaiming_normal_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    569\u001b[0m std \u001b[38;5;241m=\u001b[39m gain \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(fan)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "  \"data_folder\": \"data\",\n",
    "  \"train_batch_size\": 64,\n",
    "  \"val_batch_size\": 32,\n",
    "  \"test_batch_size\": 64,\n",
    "  \"val_split\": 0.2,\n",
    "  \"image_size\": 256,\n",
    "  \"classifier\": \"ResNet50\",\n",
    "  \"target_id\": \"class_id\",\n",
    "  \"n_classes\": 37,\n",
    "  \"classifier_save_path\": \"src/models/classifier_ResNet50.pth\",\n",
    "  \"train_classifier\": False,\n",
    "  \"n_epochs_cls\": 5,\n",
    "  \"learning_rate_cls\": 0.001,\n",
    "  \"cam_model\" : \"CAM\",\n",
    "  \"cam_threshold\": 0.5,\n",
    "  \"cam_visualisation_save_path\": \"results/cam/cam_examples.png\",\n",
    "  \"segmentation_model\": \"DeepLabV3\",\n",
    "  \"weakseg_model_save_path\": \"src/models/ws_cam_DeepLabV3.pth\",\n",
    "  \"train_segmentation\": True,\n",
    "  \"n_epochs_seg\": 5,\n",
    "  \"learning_rate_seg\": 0.0001,\n",
    "  \"segmentation_metrics_save_path\": \"results/cam/segmentation_metrics.csv\",\n",
    "  \"segmentation_visualisation_save_path\": \"results/cam/segmentation_examples.png\"\n",
    "}\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "SRC_PATH = os.path.join(BASE_DIR, \"src\")\n",
    "sys.path.append(SRC_PATH)\n",
    "\n",
    "from dataset import data_loading, inverse_normalize, load_data_wrapper, PseudoMaskDataset\n",
    "from cam_utils import CAMGenerator, generate_grad_cam_common, get_cam_generator\n",
    "from classification import ResNet50, ResNet101, DenseNet121, select_classifier\n",
    "\n",
    "FILE_PATH = os.path.join(BASE_DIR, \"hugo/data\")\n",
    "split = \"trainval\"\n",
    "print(f\"Looking for {split}.txt in:\", os.path.join(FILE_PATH, \"annotations\", f\"{split}.txt\"))\n",
    "print(\"Does the file exists?\", os.path.exists(os.path.join(FILE_PATH, \"annotations\", \"test.txt\")))\n",
    "\n",
    "def dice_score(output, gt_mask, threshold=0.5, eps=1e-7):\n",
    "    if not isinstance(output, torch.Tensor):\n",
    "        output = torch.from_numpy(output)\n",
    "    if not isinstance(gt_mask, torch.Tensor):\n",
    "        gt_mask = torch.from_numpy(gt_mask)\n",
    "    if output.ndim == 2:\n",
    "        output = output.unsqueeze(0).unsqueeze(0)\n",
    "    if gt_mask.ndim == 2:\n",
    "        gt_mask = gt_mask.unsqueeze(0).unsqueeze(0)\n",
    "    predicted_mask = (output > threshold).float()\n",
    "    gt_mask = gt_mask.float()\n",
    "    intersection = (predicted_mask * gt_mask).sum()\n",
    "    union = predicted_mask.sum() + gt_mask.sum()\n",
    "    dice = (2 * intersection + eps) / (union + eps)\n",
    "    return dice.item()\n",
    "\n",
    "def iou_score(output, gt_mask, threshold=0.5, eps=1e-7):\n",
    "    if not isinstance(output, torch.Tensor):\n",
    "        output = torch.from_numpy(output)\n",
    "    if not isinstance(gt_mask, torch.Tensor):\n",
    "        gt_mask = torch.from_numpy(gt_mask)\n",
    "    if output.ndim == 2:\n",
    "        output = output.unsqueeze(0).unsqueeze(0)\n",
    "    if gt_mask.ndim == 2:\n",
    "        gt_mask = gt_mask.unsqueeze(0).unsqueeze(0)\n",
    "    predicted_mask = (output > threshold).float()\n",
    "    gt_mask = gt_mask.float()\n",
    "    intersection = (predicted_mask * gt_mask).sum()\n",
    "    union = predicted_mask.sum() + gt_mask.sum() - intersection\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou.item()\n",
    "\n",
    "def pixel_accuracy(output, gt_mask, threshold=0.5):\n",
    "    if not isinstance(output, torch.Tensor):\n",
    "        output = torch.from_numpy(output)\n",
    "    if not isinstance(gt_mask, torch.Tensor):\n",
    "        gt_mask = torch.from_numpy(gt_mask)\n",
    "    if output.ndim == 2:\n",
    "        output = output.unsqueeze(0).unsqueeze(0)\n",
    "    if gt_mask.ndim == 2:\n",
    "        gt_mask = gt_mask.unsqueeze(0).unsqueeze(0)\n",
    "    predicted_mask = (output > threshold).float()\n",
    "    gt_mask = gt_mask.float()\n",
    "    correct = (predicted_mask == gt_mask).float()\n",
    "    total = torch.numel(gt_mask)\n",
    "    accuracy = correct.sum() / total\n",
    "    return accuracy.item()\n",
    "\n",
    "def precision_recall(output, gt_mask, threshold=0.5, eps=1e-7):\n",
    "    if not isinstance(output, torch.Tensor):\n",
    "        output = torch.from_numpy(output)\n",
    "    if not isinstance(gt_mask, torch.Tensor):\n",
    "        gt_mask = torch.from_numpy(gt_mask)\n",
    "    if output.ndim == 2:\n",
    "        output = output.unsqueeze(0).unsqueeze(0)\n",
    "    if gt_mask.ndim == 2:\n",
    "        gt_mask = gt_mask.unsqueeze(0).unsqueeze(0)\n",
    "    predicted_mask = (output > threshold).float()\n",
    "    gt_mask = gt_mask.float()\n",
    "    tp = (predicted_mask * gt_mask).sum()\n",
    "    predicted_positive = predicted_mask.sum()\n",
    "    actual_positive = gt_mask.sum()\n",
    "    precision = (tp + eps) / (predicted_positive + eps)\n",
    "    recall = (tp + eps) / (actual_positive + eps)\n",
    "    return precision.item(), recall.item()\n",
    "\n",
    "def eval_pseudo_masks_dice(pseudo_masks, masks):\n",
    "    return sum([dice_score(pseudo_mask, true_mask.squeeze()) for pseudo_mask, true_mask in zip(pseudo_masks, masks)])\n",
    "\n",
    "def eval_pseudo_masks_pixel_accuracy(pseudo_masks, masks):\n",
    "    return sum([pixel_accuracy(pseudo_mask, true_mask.squeeze()) for pseudo_mask, true_mask in zip(pseudo_masks, masks)])\n",
    "\n",
    "def eval_pseudo_masks_iou(pseudo_masks, masks):\n",
    "    return sum([iou_score(pseudo_mask, true_mask.squeeze()) for pseudo_mask, true_mask in zip(pseudo_masks, masks)])\n",
    "\n",
    "def eval_pseudo_masks_precision_recall(pseudo_masks, masks):\n",
    "    precisions_recalls = [precision_recall(pseudo_mask, true_mask.squeeze()) for pseudo_mask, true_mask in zip(pseudo_masks, masks)]\n",
    "    precisions = [pr[0] for pr in precisions_recalls]\n",
    "    recalls = [pr[1] for pr in precisions_recalls]\n",
    "    return sum(precisions), sum(recalls)\n",
    "\n",
    "def evaluate(pseudo_masks, mask_batch, batch_size):\n",
    "    dice = eval_pseudo_masks_dice(pseudo_masks, mask_batch) / batch_size\n",
    "    accuracy_score = eval_pseudo_masks_pixel_accuracy(pseudo_masks, mask_batch) / batch_size\n",
    "    iou = eval_pseudo_masks_iou(pseudo_masks, mask_batch) / batch_size\n",
    "    precision, recall = eval_pseudo_masks_precision_recall(pseudo_masks, mask_batch) \n",
    "    return dice, accuracy_score, iou, precision / batch_size, recall / batch_size\n",
    "\n",
    "def generate_pseudo_mask(bbox, image, variant=\"GrabCut\"):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    image_uint8 = (image * 255).clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    if variant == \"GrabCut\":\n",
    "        grabcut_mask = np.zeros(image_uint8.shape[:2], np.uint8)\n",
    "        bgdModel = np.zeros((1, 65), np.float64)\n",
    "        fgdModel = np.zeros((1, 65), np.float64)\n",
    "        cv2.grabCut(image_uint8, grabcut_mask, (x_min, y_min, x_max - x_min, y_max - y_min), bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "        mask2 = np.where((grabcut_mask == 2) | (grabcut_mask == 0), 0, 1).astype('uint8')\n",
    "        pseudo_mask = (mask2 * 255).astype(np.uint8)\n",
    "    \n",
    "    elif variant == \"Super-Pixel\":\n",
    "        image_cv = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2BGR)\n",
    "        image_cv = cv2.GaussianBlur(image_cv, (5, 5), sigmaX=1, sigmaY=1)\n",
    "        h, w = image_cv.shape[:2]\n",
    "        region_size = int(np.sqrt((h * w) / 200))\n",
    "        slic = cv2.ximgproc.createSuperpixelSLIC(image_cv, cv2.ximgproc.SLIC, region_size=region_size, ruler=10)\n",
    "        slic.iterate(10)\n",
    "        segments = slic.getLabels()\n",
    "        pseudo_mask = np.zeros_like(segments, dtype=np.uint8)\n",
    "        for label in np.unique(segments):\n",
    "            sp_mask = (segments == label)\n",
    "            coords = np.column_stack(np.where(sp_mask))\n",
    "            if len(coords) == 0:\n",
    "                continue\n",
    "            y_center, x_center = coords.mean(axis=0)\n",
    "            if (x_center >= x_min and x_center <= x_max and y_center >= y_min and y_center <= y_max):\n",
    "                pseudo_mask[sp_mask] = 1\n",
    "        pseudo_mask = (pseudo_mask * 255).astype(np.uint8)\n",
    "    \n",
    "    return pseudo_mask\n",
    "\n",
    "def generate_pseudo_masks(bboxs, images, variant=\"GrabCut\"):\n",
    "    return [generate_pseudo_mask(bbox, inverse_normalize(image).permute(1, 2, 0).cpu().numpy(), variant=variant) for bbox, image in zip(bboxs, images)]\n",
    "\n",
    "def generate_bbox(cam, image):\n",
    "    image_uint8 = (image * 255).clip(0, 255).astype(np.uint8)\n",
    "    threshold = 0.5\n",
    "    binary_mask = cam > threshold\n",
    "    coords = np.column_stack(np.where(binary_mask))\n",
    "    if coords.size != 0:\n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "        return (x_min, y_min, x_max, y_max)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_bboxs(cams, images):\n",
    "    return [generate_bbox(cam, inverse_normalize(image).permute(1, 2, 0).cpu().numpy()) for cam, image in zip(cams, images)]\n",
    "\n",
    "def generate_cam(image):\n",
    "    image_uint8 = (image * 255).clip(0, 255).astype(np.uint8)\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.eval()\n",
    "    target_layer = model.layer4[-1]\n",
    "    activations = None\n",
    "    gradients = None\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        nonlocal activations\n",
    "        activations = output.detach()\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_output[0].detach()\n",
    "\n",
    "    target_layer.register_forward_hook(forward_hook)\n",
    "    target_layer.register_backward_hook(backward_hook)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    pil_img = Image.fromarray(image_uint8)\n",
    "    input_tensor = preprocess(pil_img).unsqueeze(0)\n",
    "    output = model(input_tensor)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    model.zero_grad()\n",
    "    output[0, pred_class].backward()\n",
    "    weights = torch.mean(gradients, dim=(2, 3))\n",
    "    cam = torch.zeros(activations.shape[2:], dtype=torch.float32)\n",
    "    for i, w in enumerate(weights[0]):\n",
    "        cam += w * activations[0, i, :, :]\n",
    "    cam = F.relu(cam)\n",
    "    cam = cam - cam.min()\n",
    "    if cam.max() != 0:\n",
    "        cam = cam / cam.max()\n",
    "    cam = cam.unsqueeze(0).unsqueeze(0)\n",
    "    cam = F.interpolate(cam, size=(pil_img.size[1], pil_img.size[0]), mode='bilinear', align_corners=False)\n",
    "    cam = cam.squeeze().cpu().numpy()\n",
    "    return cam\n",
    "\n",
    "def generate_cams(images):\n",
    "    return [generate_cam(inverse_normalize(image).permute(1, 2, 0).cpu().numpy()) for image in images]\n",
    "\n",
    "def visualise_results(image, bbox, mask, pseudo_mask, variant=\"GrabCut\", save_path=None):\n",
    "    # Convert tensors to numpy arrays\n",
    "    image_np = inverse_normalize(image).permute(1, 2, 0).cpu().numpy()\n",
    "    mask_np = mask.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Convert numpy arrays to PIL Images\n",
    "    img_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
    "    mask_pil = Image.fromarray((mask_np * 255).astype(np.uint8)).convert('L').convert('RGB')\n",
    "    pseudo_mask_pil = Image.fromarray(pseudo_mask).convert('L').convert('RGB')\n",
    "    \n",
    "    # Create composite image\n",
    "    composite = Image.new('RGB', (img_pil.width * 3, img_pil.height))\n",
    "    \n",
    "    # Draw bounding box on original image\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    draw.rectangle([x_min, y_min, x_max, y_max], outline='red', width=3)\n",
    "    \n",
    "    # Paste images into composite\n",
    "    composite.paste(img_pil, (0, 0))\n",
    "    composite.paste(mask_pil, (img_pil.width, 0))\n",
    "    composite.paste(pseudo_mask_pil, (img_pil.width * 2, 0))\n",
    "    \n",
    "    # Add labels\n",
    "    draw = ImageDraw.Draw(composite)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "        \n",
    "    draw.text((10, 10), \"Image with Bounding Box\", fill='white', font=font)\n",
    "    draw.text((img_pil.width + 10, 10), \"Ground Truth Mask\", fill='white', font=font)\n",
    "    draw.text((img_pil.width * 2 + 10, 10), f\"Pseudo Mask {variant}\", fill='white', font=font)\n",
    "    \n",
    "    if save_path:\n",
    "        composite.save(save_path)\n",
    "    return composite\n",
    "\n",
    "def save_pseudo_masks(pseudo_masks, batch_idx, output_dir):\n",
    "    for idx, pseudo_mask in enumerate(pseudo_masks):\n",
    "        pseudo_mask_np = np.squeeze(pseudo_mask)\n",
    "        if pseudo_mask_np.max() <= 1.0:\n",
    "            pseudo_mask_np = (pseudo_mask_np * 255).astype(np.uint8)\n",
    "        else:\n",
    "            pseudo_mask_np = pseudo_mask_np.astype(np.uint8)\n",
    "        file_name = os.path.join(output_dir, f\"pseudo_mask_batch{batch_idx}_idx{idx}.png\")\n",
    "        Image.fromarray(pseudo_mask_np).save(file_name)\n",
    "    print(f'Images saved to file')\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data_wrapper(config=config)\n",
    "\n",
    "total_dice = 0.0\n",
    "total_accuracy = 0.0\n",
    "total_iou = 0.0\n",
    "total_precision = 0.0\n",
    "total_recall = 0.0\n",
    "n_batches = len(train_loader)\n",
    "batch_size = 64\n",
    "output_dir = \"saved_pseudo_masks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for batch_idx, (image_batch, mask_batch, info_batch) in enumerate(train_loader, start=1):\n",
    "    print(f\"Batch number: {batch_idx}\")\n",
    "    cams = generate_cams(image_batch)\n",
    "    bboxs = generate_bboxs(cams, image_batch)\n",
    "    pseudo_masks = generate_pseudo_masks(bboxs, image_batch, variant=\"GrabCut\")\n",
    "    dice, accuracy_score, iou, precision, recall = evaluate(pseudo_masks, mask_batch, batch_size)\n",
    "    print(f'Dice score is {dice} || Pixel score is {accuracy_score} || IOU score is {iou} || Precision score is {precision} || Recall score is {recall}')\n",
    "    total_dice += dice\n",
    "    total_accuracy += accuracy_score\n",
    "    total_iou += iou\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    visualise_results(image_batch[0], bboxs[0], mask_batch[0], pseudo_masks[0], variant=\"GrabCut\")\n",
    "    save_pseudo_masks(pseudo_masks, batch_idx, output_dir)\n",
    "\n",
    "print(f'Average dice score per image is {total_dice / n_batches}')\n",
    "print(f'Average accuracy score per image is {total_accuracy / n_batches}')\n",
    "print(f'Average IOU score per image is {total_iou / n_batches}')\n",
    "print(f'Average precision score per image is {total_precision / n_batches}')\n",
    "print(f'Average recall score per image is {total_recall / n_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
